Natural Language Dashboard Generator
Clave Engineering Take-Home Assessment

A natural language analytics platform for restaurant data. This system ingests fragmented, "messy" data from multiple POS and delivery providers, normalizes it into a canonical schema, and provides a GPT-powered interface for querying insights.
ðŸš€ Setup & Installation

1. Prerequisites

   Node.js (v18+)

   Python 3.9+ (for data ingestion)

   Supabase Account (PostgreSQL + Auth)

   OpenAI API Key (for the LangGraph agent)

2. Database Setup (Supabase)

   Create a new Supabase project.

   Run the contents of schema.sql in the Supabase SQL Editor to create base tables (locations, orders, order_items).

   Run the contents of views.sql to create the "Gold Layer" analytics views (v_orders_enriched, v_order_items_derived).

   Apply the following indexes for performance:
   SQL

   CREATE INDEX idx_orders_location_time ON orders (location_id, ordered_at);
   CREATE INDEX idx_order_items_order ON order_items (order_id);
   CREATE INDEX idx_order_items_norm_name ON order_items (normalized_name);

3. Environment Variables

Create a .env file in the root directory:
Code snippet

NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_key
OPENAI_API_KEY=your_openai_key

4. Data Ingestion

Install Python dependencies and run the ingestion scripts to populate your database:
Bash

pip install -r requirements.txt
python ingest_toast.py
python ingest_doordash.py
python ingest_square.py

ðŸ“Š Database Schema & Data Normalization
Schema Design

The architecture uses a normalized Postgres schema designed for high-performance aggregations.
Cleaning & Normalization Approach

    Deduplication: Uses a stable source key (source, source_order_id) to ensure idempotent re-runs.

    Monetary Precision: All currency is stored as integer cents (bigint) to avoid floating-point rounding errors.

        item_sales_cents: Net sales (pre-tax/tip).

        total_cents: Gross revenue (includes tax, tip, fees).

    Text Normalization: Centralized in normalize.py. It strips emojis, collapses whitespace, and standardizes casing for both item names and categories.

    Canonical Categories: Maps disparate source categories into four predictable buckets: Beverages, Food, Desserts, and Entrees (default).

ðŸ¤– AI Query Parsing & Logic

The dashboard uses a LangGraph-based Agentic Workflow to translate natural language into structured data visualizations.
The Agent Pipeline

    Planner (GPT-4o-mini): Analyzes the user query against known locations and dates. It outputs a Plan (JSON) containing one or more actions.

    Executor: Validates the plan and performs the "Truth Layer" logic.

        Self-Correction: If a user asks for a single day, the executor automatically injects an hourly breakdown even if the LLM missed it.

        Deterministic Routing: Common queries (like "DoorDash totals") bypass the LLM for speed and 100% accuracy.

    Output: Returns a union of widgets (Bar, Line, Pie, Metric, AOV, or Table).

UI Rendering

The frontend (Next.js) dynamically renders components based on the widget type:

    Metric Cards: For big-picture numbers.

    AOV Charts: Specialized logic for Average Order Value by location.

    Summary Block: An "answer-first" text summary generated by the executor to highlight peaks and totals.

ðŸ§  Design Decisions & Tradeoffs

    View-Based Analytics: I chose to use Postgres Views as a "Gold Layer." This keeps the application logic simple; the LLM queries a clean view rather than performing complex multi-table joins.

    Integer Math: Storing cents instead of decimals is a non-negotiable decision for financial integrity in restaurant tech.

    LangGraph over Simple Chains: Using a graph allows for future expansion (like multi-step reasoning or clarification loops) which a simple prompt-to-SQL chain lacks.

    Safe JSON Parsing: Implemented a fallback parser for the LLM to handle cases where the model wraps JSON in markdown blocks.

ðŸ›  Future Improvements

    Caching Layer: Implement Redis caching for common aggregate queries to reduce Supabase/LLM hits.

    SQL Generation: Move from a fixed "Action" executor to a controlled Text-to-SQL approach for more "ad-hoc" flexibility.

    Enhanced Item Mapping: Use fuzzy matching or LLM-based clustering to group similar items (e.g., "Coke" and "Coca-Cola") more accurately.

    Multi-Turn Memory: Enhance the agent's ability to remember context across more than two turns of conversation.
