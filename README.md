# Natural Language Dashboard Generator

## Clave Engineering Take-Home Assessment

A natural language analytics platform for restaurant data. This system ingests fragmented, **messy data** from multiple POS and delivery providers, normalizes it into a **canonical schema**, and provides a **GPT-powered interface** for querying insights and generating dashboards.

---

## ‚ú® What this does

- Ingests data from multiple sources (POS + delivery providers)
- Normalizes raw JSON into a consistent Postgres schema
- Exposes ‚ÄúGold Layer‚Äù analytics views for clean querying
- Uses a LangGraph-based agent to convert natural language ‚Üí structured ‚Äúwidget‚Äù outputs
- Renders widgets dynamically in a Next.js UI (charts, tables, metric cards)

---

## üöÄ Setup & Installation

### 1) Prerequisites

- **Node.js** v18+
- **Python** 3.9+ (for data ingestion)
- **Supabase** account (Postgres + Auth)
- **OpenAI API Key** (for the LangGraph agent)

---

### 2) Database Setup (Supabase)

1. Create a new Supabase project.
2. Run the contents of `schema.sql` in the Supabase SQL Editor to create base tables:
   - `locations`
   - `orders`
   - `order_items`
3. Run the contents of `views.sql` to create the ‚ÄúGold Layer‚Äù analytics views:
   - `v_orders_enriched`
   - `v_order_items_derived`
4. Apply the following indexes for performance:

```sql
CREATE INDEX idx_orders_location_time ON orders (location_id, ordered_at);
CREATE INDEX idx_order_items_order ON order_items (order_id);
CREATE INDEX idx_order_items_norm_name ON order_items (normalized_name);
```

---

### 3) Environment Variables

Create a `.env` file in the project root:

```bash
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_key
OPENAI_API_KEY=your_openai_key
```

---

### 4) Data Ingestion

Install Python dependencies and run ingestion scripts to populate the database:

```bash
pip install -r requirements.txt
python ingest_toast.py
python ingest_doordash.py
python ingest_square.py
```

---

## üìä Database Schema & Data Normalization

### Schema Design

The architecture uses a normalized Postgres schema designed for high-performance aggregations.

### Cleaning & Normalization Approach

- **Deduplication / Idempotency**  
  Uses a stable source key `(source, source_order_id)` to ensure ingestion can be re-run safely without creating duplicates.

- **Monetary Precision**  
  All currency is stored as integer cents (`bigint`) to avoid floating-point rounding errors:

  - `item_sales_cents`: Net sales (pre-tax/tip)
  - `total_cents`: Gross revenue (includes tax, tip, fees)

- **Text Normalization**  
  Centralized in `normalize.py`:

  - strips emojis
  - collapses whitespace
  - standardizes casing for item names and categories

- **Canonical Categories**  
  Maps disparate source categories into predictable buckets:
  - `Beverages`
  - `Food`
  - `Desserts`
  - `Entrees` (default)

---

## ü§ñ AI Query Parsing & Visualization Logic

The dashboard uses a **LangGraph-based Agentic Workflow** to translate natural language into structured data visualizations.

### Agent Pipeline

1. **Planner (GPT-4o-mini)**  
   Analyzes the user query against known locations and dates. Outputs a `Plan` (JSON) containing one or more actions.

2. **Executor (Truth Layer)**  
   Validates the plan and executes deterministic logic:

   - **Self-correction:** If the user asks for a single day, the executor injects an hourly breakdown even if the LLM missed it.
   - **Deterministic routing:** Common queries (e.g., ‚ÄúDoorDash totals‚Äù) bypass the LLM for speed and accuracy.

3. **Output**  
   Returns a union of widgets:
   - `Bar`
   - `Line`
   - `Pie`
   - `Metric`
   - `AOV`
   - `Table`

---

## üß© UI Rendering

The frontend (Next.js) dynamically renders components based on widget type:

- **Metric Cards**: Big-picture numbers
- **AOV Charts**: Specialized logic for Average Order Value by location
- **Summary Block**: ‚ÄúAnswer-first‚Äù text summary generated by the executor to highlight peaks and totals

---

## üß† Design Decisions & Tradeoffs

- **View-Based Analytics (Gold Layer)**  
  Postgres Views keep application logic simple; the LLM queries clean views rather than building complex joins in prompts.

- **Integer Math for Money**  
  Storing cents instead of decimals is non-negotiable for financial correctness in restaurant analytics.

- **LangGraph over Simple Chains**  
  A graph supports future expansion (multi-step reasoning, clarifying questions, tool routing) better than a single prompt-to-SQL chain.

- **Safe JSON Parsing**  
  Includes a fallback parser for cases where the model wraps JSON in markdown code blocks.

---

## üõ† Future Improvements

- **Caching Layer**  
  Add Redis caching for common aggregate queries to reduce Supabase + LLM calls.

- **Controlled Text-to-SQL**  
  Move from fixed ‚ÄúAction‚Äù execution to constrained SQL generation for more ad-hoc flexibility.

- **Enhanced Item Mapping**  
  Use fuzzy matching or clustering to group similar items (e.g., ‚ÄúCoke‚Äù and ‚ÄúCoca-Cola‚Äù) more accurately.

- **Multi-Turn Memory**  
  Improve the agent‚Äôs ability to retain context across more than two conversation turns.

---
